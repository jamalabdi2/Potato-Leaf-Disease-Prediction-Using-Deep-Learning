# -*- coding: utf-8 -*-
"""Potato_Leaf_Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQ-OZJYuNmboukwAksWLddcPVdeSnjNH
"""

#!pip install opendatasets

dataset_url = 'https://www.kaggle.com/datasets/arjuntejaswi/plant-village'
import opendatasets
opendatasets.download(dataset_url)

#filter the dataset to get only potato data
import shutil
import os
BASE_DIR = os.getcwd()
main_dir = '/content/plant-village/PlantVillage'
directories = os.listdir('/content/plant-village/PlantVillage')

dir_to_remove = []
for directory in directories:
  if not directory.startswith('Potato'):
    dir_to_remove.append(directory)

for dir in dir_to_remove:
  fullpath = os.path.join(main_dir,dir)
  shutil.rmtree(fullpath)

#!pip install split-folders[full]
import splitfolders
splitfolders.ratio('/content/plant-village/PlantVillage',output='dataset',ratio =(.7,.1,.2) )

import numpy as np
import pandas as pd
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Dense,Conv2D,MaxPool2D,Flatten,InputLayer
from keras.utils import image_dataset_from_directory
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import img_to_array,load_img
import warnings
warnings.filterwarnings('ignore')

#load the images
DIRECTORY_NAME = '/content/plant-village/PlantVillage'
IMAGE_SIZE = 256
BATCH_SIZE = 32
dataset = image_dataset_from_directory(
    DIRECTORY_NAME,
    image_size = (IMAGE_SIZE,IMAGE_SIZE),
    batch_size = BATCH_SIZE,
    shuffle=True)

class_names =dataset.class_names

#create a numpy iterator,take a batch 
# a batch containing 32 images and their labels
dataset_numpy_iterator = dataset.as_numpy_iterator()
batch = dataset_numpy_iterator.next()
image,labels = batch

image[0].shape

labels

plt.figure(figsize=(8,8))
for index in range(9):
  plt.subplot(3,3,index+1)
  plt.imshow(image[index].astype('uint8'))
  plt.title(f'{class_names[labels[index]]}')
  plt.tight_layout()
  plt.axis('off')

plt.show()

DATASET_SIZE = len(dataset)
TRAINING_SIZE = round(DATASET_SIZE*0.8)
TESTING_SIZE =  round(DATASET_SIZE*0.1)
VALIDATION_SIZE = round(DATASET_SIZE*0.1)
print('Dataset_size: ',DATASET_SIZE)
print('Training size: ',TRAINING_SIZE)
print('Testing size: ',TESTING_SIZE)
print('Validation size: ',VALIDATION_SIZE)

training_dataset = dataset.take(TRAINING_SIZE)
testing_dataset = dataset.skip(TRAINING_SIZE).take(TESTING_SIZE)
validation_dataset = dataset.skip(TRAINING_SIZE+TESTING_SIZE).take(VALIDATION_SIZE)

INPUT_SHAPE = (256, 256, 3)
N_CLASSES = 3

def create_generator(directory):
    datagen = ImageDataGenerator(
        rescale = 1./255,
        horizontal_flip = True,
        rotation_range = 10   
    )
    generator = datagen.flow_from_directory(
        directory,
        target_size = (IMAGE_SIZE,IMAGE_SIZE),
        batch_size = BATCH_SIZE,
        class_mode = 'sparse'
    )
    return generator

train_generator = create_generator('/content/dataset/train')
validation_generator = create_generator('/content/dataset/val')
testing_generator = create_generator('/content/dataset/test')

#train neural network
model = Sequential([
    InputLayer(input_shape =INPUT_SHAPE),

    Conv2D(256,(3,3),activation = 'relu'),
    MaxPool2D((2,2)),
    Conv2D(128,(3,3),activation = 'relu'),
    MaxPool2D((2,2)),
    Conv2D(64,(3,3),activation = 'relu'),
    MaxPool2D((2,2)),
    Conv2D(32,(3,3),activation = 'relu'),
    MaxPool2D((2,2)),

    Flatten(),
    Dense(64,activation ='relu'),
    Dense(N_CLASSES,activation = 'softmax')
    
])

model.summary()

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

no_images_in_train = 1506
image_num = 32
step_per_epochs = int(no_images_in_train/image_num)
validation_steps = int(215/32)

model_history = model.fit(train_generator,
          steps_per_epoch = step_per_epochs,
          batch_size = 32,
          validation_data = validation_generator,
          validation_steps = validation_steps,
          verbose = 1,
          epochs = 20  
          )

training_history_df= pd.DataFrame(model_history.history)
training_history_df

accuracy_df = training_history_df[['accuracy','val_accuracy']]
loss_df = training_history_df[['loss','val_loss']]
plt.figure(figsize=(8,8))
accuracy_df.plot() 
plt.title('Accuracy Report')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

loss_df.plot()
plt.title('Loss Report')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

loss,accuracy = model.evaluate(testing_generator)
print('Model Loss: ',loss)
print('Model Accuracy: ',round(accuracy,2))

prediction = model.predict(testing_generator)

prediction_labels = [np.argmax(pred) for pred in prediction]
actual_labels = testing_generator.labels 
prediction_df = pd.DataFrame({'Actual Label':actual_labels,'Predicted Label':test_labels})
prediction_df

#predictive system
def predict_result(model,image):
  img = img_to_array(image)
  input_array = tf.expand_dims(img,0)
  new_prediction = model.predict(input_array)
  predicted_class = class_names[np.argmax(new_prediction[0])]
  confidence = round(100*(np.max(prediction[0])),2)
  
  return  predicted_class,confidence

predicted_label,confidence = predict_result(model,image[0])
print(predicted_label,confidence)

#sample images
images,labels = testing_generator.next()
plt.figure(figsize=(10,10))
for i in range(9):
  plt.subplot(3,3,i+1)
  plt.imshow(images[i])
  #make prediction
  predicted_class,confidence = predict_result(model,images[i])
  actual_class = class_names[int(labels[i])]
  plt.tight_layout()
  plt.axis('off')
  plt.title(f'Actual: {actual_class},\nPredicted: {predicted_class},\nConfidence: {confidence}')

plt.show()

#save the model
model.save('potato.h5')